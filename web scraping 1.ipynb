{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393b41ec",
   "metadata": {},
   "source": [
    "# Write a python program to display all the header tags from‘en.wikipedia.org/wiki/Main_Page’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d9d3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list all the header tags :\n",
      "\n",
      "<h1 class=\"firstHeading\" id=\"firstHeading\">Main Page</h1>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n",
      "\n",
      "<h2>Navigation menu</h2>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-personal-label\">\n",
      "<span>Personal tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n",
      "<span>Namespaces</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"Change language variant\" class=\"vector-menu-heading\" id=\"p-variants-label\">\n",
      "<span>Variants</span>\n",
      "<span class=\"vector-menu-checkbox-expanded\">expanded</span>\n",
      "<span class=\"vector-menu-checkbox-collapsed\">collapsed</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-views-label\">\n",
      "<span>Views</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-cactions-label\">\n",
      "<span>More</span>\n",
      "<span class=\"vector-menu-checkbox-expanded\">expanded</span>\n",
      "<span class=\"vector-menu-checkbox-collapsed\">collapsed</span>\n",
      "</h3>\n",
      "\n",
      "<h3>\n",
      "<label for=\"searchInput\">Search</label>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-navigation-label\">\n",
      "<span>Navigation</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-interaction-label\">\n",
      "<span>Contribute</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-tb-label\">\n",
      "<span>Tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n",
      "<span>Print/export</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-wikibase-otherprojects-label\">\n",
      "<span>In other projects</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-lang-label\">\n",
      "<span>Languages</span>\n",
      "</h3>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Main_page')\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "titles = bs.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "print('list all the header tags :', *titles, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee654efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff0e0f0c",
   "metadata": {},
   "source": [
    "# Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. Name, IMDB rating, Year of \n",
    "release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee269b70",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-20f48b0fd3ef>, line 79)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-20f48b0fd3ef>\"\u001b[1;36m, line \u001b[1;32m79\u001b[0m\n\u001b[1;33m    'return' movieData\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "\n",
    "url1 = \"https://www.imdb.com/search/title?count=100&title_type=feature,tv_series&ref_=nv_wl_img_2\"\n",
    "\n",
    "class IMDB(object):\n",
    "    \"\"\"docstring for IMDB\"\"\"\n",
    "    def _init_(self, url):\n",
    "        super(IMDB, self)._init_()\n",
    "        page = get(url)\n",
    "        \n",
    "        self.soup = BeautifulSoup(page.content, 'lxml')\n",
    "        \n",
    "    def articleTitle(self):\n",
    "        return self.soup.find(\"h1\", class_=\"header\").text.replace(\"\\n\",\"\")\n",
    "    \n",
    "    def bodyContent(self):\n",
    "        content = self.soup.find(id=\"main\")\n",
    "        return content.find_all(\"div\", class_=\"lister-item mode-advanced\")\n",
    "    \n",
    "    def movieData(self):\n",
    "        movieFreame = self.bodyContent()\n",
    "        movieTitle = []\n",
    "        movieDate = []\n",
    "        movieRunTime = []\n",
    "        movieGenre = []\n",
    "        movieRating = []\n",
    "        movieScore = []\n",
    "        movieDescription = []\n",
    "        movieDirector = []\n",
    "        movieStars = []\n",
    "        movieVotes = []\n",
    "        movieGross = []\n",
    "        for movie in movieFrame:\n",
    "            movieFirstLine = movie.find(\"h3\", class_=\"lister-item-header\")\n",
    "            movieTitle.append(movieFirstLine.find(\"a\").text)\n",
    "            movieDate.append(re.sub(r\"[()]\",\"\", movieFirstLine.find_all(\"span\")[-1].text))\n",
    "            try:\n",
    "                    movieRunTime.append(movie.find(\"span\", class_=\"runtime\").text[:-4])\n",
    "            except:\n",
    "                    movieRunTime.append(np.nan)\n",
    "            movieGenre.append(movie.find(\"span\", class_=\"genre\").text.rstrip().replace(\"\\n\",\"\").split(\",\"))\n",
    "            try:\n",
    "                    movieRating.append(movie.find(\"strong\").text)\n",
    "            except:\n",
    "                    movieRating.append(np.nan)\n",
    "            try:\n",
    "                    movieScore.append(movie.find(\"span\", class_=\"metascore unfavorable\").text.rstrip())\n",
    "            except:\n",
    "                    movieScore.append(np.nan)\n",
    "            movieDescription.append(movie.find_all(\"p\", class_=\"text-muted\")[-1].text.lstrip())\n",
    "            movieCast = movie.find(\"p\", class_=\"\")\n",
    "            \n",
    "            try:\n",
    "                    casts = movieCast.text.replace(\"\\n\",\"\").splict('|')\n",
    "                    casts = [x.strip() for x in casts]\n",
    "                    casts = [casts[i].replaced(j, \"\") for i,j in enumerate([\"Director:\", \"Stars:\"])]\n",
    "                    movieDirector.append(casts[0])\n",
    "                    movieStars.append([x.strip() for x in casts[1].split(\",\")])\n",
    "            except:\n",
    "                    casts = movieCast.text.replace(\"\\n\",\"\").strip()\n",
    "                    movieDirector.append(np.nan)\n",
    "                    movieStars.append([x.strip() for x in casts.split(\",\")])\n",
    "                    \n",
    "            movieNumbers = movie.find_all(\"span\", attrs={\"name\": \"nv\"})\n",
    "            \n",
    "            if len(movieNumbers) == 2:\n",
    "                    movieVotes.append(movieNumbers[0].text)\n",
    "                    movieGross.append(movieNumbers[1].text)\n",
    "            elif len(movieNumbers) == 1:\n",
    "                    movieVotes.append(movieNumbers[0].text)\n",
    "                    movieGross.append(np.nan)\n",
    "            else:\n",
    "                    movieVotes.append(np.nan)\n",
    "                    movieGross.append(np.nan)\n",
    "    movieData = [movieTitle, movieDate, movieRunTime, movieGenre, movieRating, movieScore, movieDescription,\n",
    "                                            movieDirector, movieStars, movieVotes, movieGross]\n",
    "    \n",
    "    return movieData\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4c771",
   "metadata": {},
   "source": [
    "# Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. Name, IMDB rating, Year \n",
    "of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88123219",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rating'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-1aeff0d3e09e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmovie\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     print(movie['place'], '-', movie['movie_title'], '('+movie['year']+\n\u001b[1;32m---> 48\u001b[1;33m           ') -', 'Staring:', movie['star_cast'], movie['rating'])\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'rating'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Downloading imdb top 250 movie's data\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "movies = soup.select('td.titleColumn')\n",
    "links = [a.attrs.get('href') for a in soup.select('td.titleColumn a')]\n",
    "crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
    "\n",
    "ratings = [b.attrs.get('data-value')\n",
    "           for b in soup.select('td.posterColumn span[name=ir]')]\n",
    "\n",
    "votes = [b.attrs.get('data-value')\n",
    "         for b in soup.select('td.ratingColumn strong')]\n",
    "\n",
    "list = []\n",
    "\n",
    "# create a empty list for storing\n",
    "# movie information\n",
    "list = []\n",
    "\n",
    "# Iterating over movies to extract\n",
    "# each movie's details\n",
    "for index in range(0, len(movies)):\n",
    "    \n",
    "    # Separating movie into: 'place',\n",
    "    # 'title', 'year'\n",
    "    movie_string = movies[index].get_text()\n",
    "    movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "    movie_title = movie[len(str(index))+1:-7]\n",
    "    year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "    place = movie[:len(str(index))-(len(movie))]\n",
    "    data = {\"movie_title\": movie_title,\n",
    "           \"year\": year,\n",
    "           \"place\": place,\n",
    "           \"star_cast\": crew[index],\n",
    "           \"vote\": votes[index],\n",
    "           \"link\": links[index]}\n",
    "    list.append(data)\n",
    "    \n",
    "# printing movie details with its rating.\n",
    "for movie in list:\n",
    "    print(movie['place'], '-', movie['movie_title'], '('+movie['year']+\n",
    "          ') -', 'Starring:', movie['star_cast'], movie['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd49a10",
   "metadata": {},
   "source": [
    "# Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have toscrape:\n",
    "i) Top 10 ODI teams in men’s cricket along with the records for matches, points andrating.\n",
    "ii) Top 10 ODI Batsmen in men along with the records of their team and rating.\n",
    "iii) Top 10 ODI bowlers along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225f3c1",
   "metadata": {},
   "source": [
    "# Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have toscrape:\n",
    "i) Top 10 ODI teams in women’s cricket along with the records for matches, points andrating.\n",
    "ii) Top 10 women’s ODI players along with the records of their team and rating.\n",
    "iii) Top 10 women’s ODI all-rounder along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "840aa5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 -> https://www.icc-cricket.com/rankings/mens/player-rankings/test/batting\n",
      "200 -> https://www.icc-cricket.com/rankings/mens/player-rankings/test/bowling\n",
      "200 -> https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\n",
      "200 -> https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\n",
      "200 -> https://www.icc-cricket.com/rankings/mens/player-rankings/t20i/batting\n",
      "200 -> https://www.icc-cricket.com/rankings/mens/player-rankings/t20i/bowling\n",
      "200 -> https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\n",
      "200 -> https://www.icc-cricket.com/rankings/womens/player-rankings/t20i/batting\n",
      "200 -> https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling\n",
      "200 -> https://www.icc-cricket.com/rankings/womens/player-rankings/t20i/bowling\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "urls = [\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/test/batting\",\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/test/bowling\",\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\",\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\",\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/t20i/batting\",\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/t20i/bowling\",\n",
    "\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\",\n",
    "\"https://www.icc-cricket.com/rankings/womens/player-rankings/t20i/batting\",\n",
    "\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling\",\n",
    "\"https://www.icc-cricket.com/rankings/womens/player-rankings/t20i/bowling\",\n",
    "]\n",
    "\n",
    "final_result_file_name = \"All Ranking List.csv\"\n",
    "final_column_names = [\"Ranking Type\", \"Position\", \"Player Name\", \"Team Name\", \"Rating\", \"Career Best Rating\", \"Crawl URL\"]\n",
    "pd.DataFrame(columns=final_column_names).to_csv(final_result_file_name, sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "for url in urls:\n",
    "    request_object = requests.get(url, headers=headers)\n",
    "    html_content = request_object.text\n",
    "    print(request_object.status_code, \"->\", url)\n",
    "    soup_object = BeautifulSoup(html_content, \"lxml\")\n",
    "    for element in soup_object.select('[class=\"ranking-pos up\"], [class=\"ranking-pos down\"]'):\n",
    "        element.replace_with(BeautifulSoup(\"<\" + element.name + \"></\" + element.name + \">\", \"html.parser\"))\n",
    "\n",
    "    ranking_type = soup_object.select_one(\".rankings-block__title-container > h4\").text\n",
    "\n",
    "    result_file_name = ranking_type + \".csv\"\n",
    "    column_names = [\"Position\", \"Player Name\", \"Team Name\", \"Rating\", \"Career Best Rating\", \"Crawl URL\"]\n",
    "    pd.DataFrame(columns=column_names).to_csv(result_file_name, sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    for element in soup_object.select('table[class=\"table rankings-table\"] tr'):\n",
    "        if(element.find(\"th\")):\n",
    "            continue\n",
    "        data_dict = dict()\n",
    "        data_dict[\"Crawl URL\"] = url\n",
    "        data_dict[\"Ranking Type\"] = ranking_type\n",
    "        if(element.select_one('[class*=\"position\"]')):\n",
    "            data_dict[\"Position\"] = element.select_one('[class*=\"position\"]').text\n",
    "        for player_name in (element.select('a[href*=\"/player-rankings\"]')):\n",
    "            if(player_name.text.strip()):\n",
    "                data_dict[\"Player Name\"] = player_name.text\n",
    "        if(element.select_one('[class^=\"flag-15\"]')):\n",
    "            data_dict[\"Team Name\"] = element.select_one('[class^=\"flag-15\"]')[\"class\"][-1]\n",
    "        if(element.select_one('[class$=\"rating\"]')):\n",
    "            data_dict[\"Rating\"] = element.select_one('[class$=\"rating\"]').text\n",
    "        if(element.select_one('td.u-hide-phablet')):\n",
    "            data_dict[\"Career Best Rating\"] = element.select_one('td.u-hide-phablet').text\n",
    "        for key in data_dict.keys():\n",
    "            data_dict[key] = re.sub(r\"\\s+\", \" \", data_dict[key])\n",
    "            data_dict[key] = data_dict[key].strip()\n",
    "        pd.DataFrame([data_dict], columns=column_names).to_csv(result_file_name, sep=\"\\t\", index=False, header=False, encoding=\"utf-8\", mode=\"a\")\n",
    "        pd.DataFrame([data_dict], columns=final_column_names).to_csv(final_result_file_name, sep=\"\\t\", index=False, header=False, encoding=\"utf-8\", mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b047b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56db0445",
   "metadata": {},
   "source": [
    "# Write a python program to scrape details of all the mobile phones under Rs. 20,000 listed on Amazon.in. The \n",
    "scraped data should include Product Name, Price, Image URL and Average Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85b7134d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-21-702564eb0ee4>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-702564eb0ee4>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    'Mozilla/5.0 (X11; Linux x86_64)\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    " \n",
    "def main(URL):\n",
    "    # opening our output file in append mode\n",
    "    File = open(\"out.csv\", \"a\")\n",
    " \n",
    "    # specifying user agent, You can use other user agents\n",
    "    # available on the internet\n",
    "    HEADERS = ({'User-Agent':\n",
    "                'Mozilla/5.0 (X11; Linux x86_64)\n",
    "                    AppleWebKit/537.36 (KHTML, like Gecko)\n",
    "                            Chrome/44.0.2403.157 Safari/537.36',\n",
    "                                'Accept-Language': 'en-US, en;q=0.5'})\n",
    " \n",
    "    # Making the HTTP Request\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    " \n",
    "    # Creating the Soup Object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    " \n",
    "    # retrieving product title\n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"span\",\n",
    "                          attrs={\"id\": 'productTitle'})\n",
    " \n",
    "        # Inner NavigableString Object\n",
    "        title_value = title.string\n",
    " \n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip().replace(',', '')\n",
    " \n",
    "    except AttributeError:\n",
    "        title_string = \"NA\"\n",
    "    print(\"product Title = \", title_string)\n",
    " \n",
    "    # saving the title in the file\n",
    "    File.write(f\"{title_string},\")\n",
    " \n",
    "    # retrieving price\n",
    "    try:\n",
    "        price = soup.find(\n",
    "            \"span\", attrs={'id': 'priceblock_ourprice'})\n",
    "                                .string.strip().replace(',', '')\n",
    "        # we are omitting unnecessary spaces\n",
    "        # and commas form our string\n",
    "    except AttributeError:\n",
    "        price = \"NA\"\n",
    "    print(\"Products price = \", price)\n",
    " \n",
    "    # saving\n",
    "    File.write(f\"{price},\")\n",
    " \n",
    "    # retrieving product rating\n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={\n",
    "                           'class': 'a-icon a-icon-star a-star-4-5'})\n",
    "                                    .string.strip().replace(',', '')\n",
    " \n",
    "    except AttributeError:\n",
    " \n",
    "        try:\n",
    "            rating = soup.find(\n",
    "                \"span\", attrs={'class': 'a-icon-alt'})\n",
    "                                .string.strip().replace(',', '')\n",
    "        except:\n",
    "            rating = \"NA\"\n",
    "    print(\"Overall rating = \", rating)\n",
    " \n",
    "    File.write(f\"{rating},\")\n",
    " \n",
    "    try:\n",
    "        review_count = soup.find(\n",
    "            \"span\", attrs={'id': 'acrCustomerReviewText'})\n",
    "                                .string.strip().replace(',', '')\n",
    " \n",
    "    except AttributeError:\n",
    "        review_count = \"NA\"\n",
    "    print(\"Total reviews = \", review_count)\n",
    "    File.write(f\"{review_count},\")\n",
    " \n",
    "    # print availablility status\n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id': 'availability'})\n",
    "        available = available.find(\"span\")\n",
    "                    .string.strip().replace(',', '')\n",
    " \n",
    "    except AttributeError:\n",
    "        available = \"NA\"\n",
    "    print(\"Availability = \", available)\n",
    " \n",
    "    # saving the availability and closing the line\n",
    "    File.write(f\"{available},\\n\")\n",
    " \n",
    "    # closing the file\n",
    "    File.close()\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "  # opening our url file to access URLs\n",
    "    file = open(\"url.txt\", \"r\")\n",
    " \n",
    "    # iterating over the urls\n",
    "    for links in file.readlines():\n",
    "        main(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db603d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3482581",
   "metadata": {},
   "source": [
    "# Write a python program to scrape house details from mentioned url. It should include house title, location, \n",
    "area, emi and price\n",
    "https://www.nobroker.in/property/sale/bangalore/Electronic%20City?type=BHK4&searchParam=W3sibGF0IjoxMi44N\n",
    "DUyMTQ1LCJsb24iOjc3LjY2MDE2OTUsInBsYWNlSWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8\n",
    "iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0=&propertyAge=0&radius=2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caadbc22",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'progressbar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-02e9a4639ef8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mprogressbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'progressbar'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import progressbar\n",
    "\n",
    "\n",
    "headers = ({'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'})\n",
    "\n",
    "# dataframe\n",
    "titles = []\n",
    "addresses = []\n",
    "rents = []\n",
    "sizes = []\n",
    "deposits = []\n",
    "furnishings = []\n",
    "property_ages = []\n",
    "available_fors = []\n",
    "immediate_possessions = []\n",
    "\n",
    "# progressbar for displaying % completion\n",
    "bar = progressbar.ProgressBar(maxval=1000)\n",
    "bar.start()\n",
    "\n",
    "# scraping through 1000 pages of nobroker website of places in Bangalore\n",
    "for page in range(1000):\n",
    "    bar.update(page+1)\n",
    "    page += 1;\n",
    "    link = \"https://www.nobroker.in/property/sale/bangalore/Electronic%20City?type=BHK4&searchParam=W3sibGF0IjoxMi44NDUyMTQ1LCJsb24iOjc3LjY2MDE2OTUslnBsYWNISWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0=&propertyAge=0&radius=2.0\"\n",
    "    response = get(link,headers=headers)\n",
    "\n",
    "    # for testing if scraping of website is allowed...\n",
    "    # print(response)\n",
    "    # print(response.text[:1000])\n",
    "\n",
    "\n",
    "    # Parsing through html page\n",
    "    html_soup = BeautifulSoup(response.text,'html.parser')\n",
    "    house_containers = html_soup.find_all('div', class_=\"card\")\n",
    "    if(house_containers != []):\n",
    "        for container in house_containers:\n",
    "            \n",
    "            try:\n",
    "                rent = container.find_all('h3')[2].find('span').text.replace(',','')\n",
    "                rent = int(''.join(itertools.takewhile(str.isdigit,rent)))\n",
    "                rents.append(int(rent))\n",
    "            except:\n",
    "                rents.append('-')\n",
    "            try:\n",
    "                size = int(container.find_all('h3')[0].find_all('span')[0].text.replace(',',''))\n",
    "                sizes.append(int(size))\n",
    "            except:\n",
    "                sizes.append('-')\n",
    "            try:\n",
    "                deposit = int(container.find_all('h3')[1].find_all('span')[0].text.replace(',',''))\n",
    "                deposits.append(int(deposit))\n",
    "            except:\n",
    "                deposits.append('-')\n",
    "            title = (container.find('div','card-header-title').find('h2').text.replace('\\n',''))\n",
    "            address = (container.find('div','card-header-title').find('h5').text.replace('\\n',''))\n",
    "            titles.append(title)\n",
    "            addresses.append(address)\n",
    "\n",
    "            furnishing = (container.find('div','detail-summary').find_all('h5')[0].text.replace('\\n',''))\n",
    "            furnishings.append(furnishing)\n",
    "    \n",
    "            property_age = (container.find('div','detail-summary').find_all('h5')[1].text.replace('\\n',''))\n",
    "            property_ages.append(property_age)\n",
    "    \n",
    "            available_for = (container.find('div','detail-summary').find_all('h5')[2].text.replace('\\n',''))\n",
    "            available_fors.append(available_for)\n",
    "    \n",
    "            immediate_possession = (container.find('div','detail-summary').find_all('h5')[3].text.replace('\\n',''))\n",
    "            immediate_possessions.append(immediate_possession)\n",
    "    else:\n",
    "        break;\n",
    "\n",
    "    time.sleep(random.randint(1,2))\n",
    "bar.finish()\n",
    "print(\"Successfully scraped {} pages containing {} properties.\".format(page,len(titles)))\n",
    "\n",
    "\n",
    "# creating dataframe to save data in .csv format\n",
    "columns = ['Title', 'Address', 'Rent(Rs)', 'Deposit(Rs)', 'Size(Acres)', 'Furnishing', 'Property age', 'Available for', 'Immediate possession']\n",
    "        \n",
    "bangalore= pd.DataFrame({'Title': titles,\n",
    "                        'Address': addresses,\n",
    "                        'Rent(Rs)': rents,\n",
    "                        'Deposit(Rs)': deposits,\n",
    "                        'Size(Acres)': sizes,\n",
    "                        'Furnishing': furnishings,\n",
    "                        'Property age': property_ages,\n",
    "                        'Available for': available_fors,\n",
    "                        'Immediate possession': immediate_possessions})[cols]\n",
    "bangalore.to_csv('bangalore_rent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e765d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b56226d",
   "metadata": {},
   "source": [
    "# Write a python program to scrape mentioned details from ‘https://www.dineout.co.in/delhi-restaurants/buffet\u0002special’ :\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc24f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
